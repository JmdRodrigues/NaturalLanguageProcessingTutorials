{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "technological-satellite",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "\n",
    "The analysis of a language requires being aware of the existing ambiguities. These are not necessarily an ambiguity because of the text used, but rather because when making a specific analysis, words that are conjugated, or words and sentence connectors, are not essential, and can be noise for the desired analysis. For instance, words with high frequency of appearance in the *English-language* are usually \"the\", \"and\", \"or\", etc...which are not relevant for most analysis. Additionally, using conjugations of verbs, or gender might not be relevant as well.\n",
    "\n",
    "Pre-processing the text involves considering all these aspects of a language, and is an utterly relevant step. The process involves cleaning the text, removing unnecessary words and punctuation, and transforming the text in its simplest form to extract relevant features and be processed correctly. In this section, the most relevant methods for text pre-processing are presented.\n",
    "\n",
    "In this notebook, the main pre-processing strategies for text are exemplified with tools such as nltk, spacy and textblob.\n",
    "These steps involve: \n",
    "\n",
    "- tokenization;\n",
    "- stop word removal;\n",
    "- lemmatization/stemming;\n",
    "- N-grams\n",
    "- parsing\n",
    "- Grammar inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-general",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization refers to the process of segmenting text in tokens. These may represent words, characters or subwords. For instance, the sentence \"I am supermotivated\" can be separated in \"[I, am, supermotivated]\" or \"[I, am, super, motivated]\". This step is very relevant to prepare the data for next processing steps. \n",
    "\n",
    "### Stop Words Removal\n",
    "\n",
    "Stop words are referred as words that are not important for processing. In fact, these words are defined as noise and can be prejudicial to the analysis. For this reason, a list of stop words exist for each language, being removed from text before next processing steps. Typically these are very common words, which might confuse the system in evaluating the similarity between documents.\n",
    "\n",
    "### Lemmatizing/Stemming\n",
    "\n",
    "As mentioned, languages use conjugation for gender, verbs, etc...which requires making variations of a word, and in some cases, with irregular conjugations, changing the root form of the word completely. Having the root form of a word is better for post-processing methodologies, since words conjugated differently will be treated as the same. Considering the sentence \"I \\textbf{like} to have something that he \\textbf{likes}\", \"like\" and \"likes\" are originated from the same root word \"like\", and should be considered as \"like\" for the benefit of improving the analysis of some methodologies. There are two ways of simplifying this text representation: Lemmatisation and Stemming.\n",
    "\n",
    " Lemmatisation is the process of grouping together the inflected forms of a word considering the lemma, that is the dictionary form (e.g: \"\\textit{to walk}\", \"\\textit{walked}\" or \"\\textit{walking}\" have the same lemma \"\\textit{walk}\".\n",
    "\n",
    " Stemming differs from Lemmatisation in the sense that the meaning is not inferred as in the case of the lemma. A stemma is the root form of a word, e.g. \"\\textit{world}\" is the stem of \"\\textit{worldwide}\" and \"\\textit{worlds}\". \n",
    " \n",
    "### N-grams\n",
    "\n",
    "Text is a sequential structure, in which the \\textit{next} word has a certain level of dependency from \\textit{previous} ones. Therefore, when designing probabilistic models, having the text structured in a way that we are able to understand these dependencies is of great relevance. In that sense, the N-grams structure was designed for this purpose. N-grams are a structured way of organizing the text by grouping \\textit{N} tokens that are followed, with a total overlap of the sequence. For instance, the sentence \"I am doing great\", organized as a bigram (2-grams), would be: [\"I am\", \"am doing\", \"doing great\"]. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-establishment",
   "metadata": {},
   "source": [
    "#### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "instrumental-resolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sentence:------------\n",
      "['I', 'am', 'super-motivated', '.']\n",
      "\n",
      "tagged tokens:------------\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('super-motivated', 'JJ'), ('.', '.')]\n",
      "\n",
      "named entities:------------\n",
      "(S I/PRP am/VBP super-motivated/JJ ./.)\n",
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n",
      "studi\n",
      "studi\n",
      "cri\n",
      "cri\n",
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n",
      "[('A', 'class'), ('class', 'is'), ('is', 'a'), ('a', 'blueprint'), ('blueprint', 'for'), ('for', 'the'), ('the', 'object'), ('object', '.')]\n",
      "[('A', 'class', 'is', 'a'), ('class', 'is', 'a', 'blueprint'), ('is', 'a', 'blueprint', 'for'), ('a', 'blueprint', 'for', 'the'), ('blueprint', 'for', 'the', 'object'), ('for', 'the', 'object', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk as nltk\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Tokenization\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "sentence = \"I am super-motivated.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(\"tokenized sentence:------------\")\n",
    "print(tokens)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(\"\\ntagged tokens:------------\")\n",
    "print(tagged)\n",
    "\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(\"\\nnamed entities:------------\")\n",
    "print(entities)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Stop Word Removal\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#from: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example_sent = \"\"\"This is a sample sentence, showing off the stop words filtration.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example_sent) \n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "\n",
    "filtered_sentence = [] \n",
    "\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "print(word_tokens) \n",
    "print(filtered_sentence) \n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Lemmatizing\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#from: https://www.guru99.com/stemming-lemmatization-python-nltk.html\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "e_words= [\"studies\", \"studying\", \"cries\", \"cry\"]\n",
    "\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)\n",
    "    \n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))\n",
    "    \n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# N-grams\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#from: https://www.pythonprogramming.in/generate-the-n-grams-for-the-given-sentence-using-nltk-or-textblob.html\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "data = 'A class is a blueprint for the object.'\n",
    "\n",
    "n_grams = ngrams(nltk.word_tokenize(data), 2)\n",
    "print([gram for gram in n_grams])\n",
    "n_grams = ngrams(nltk.word_tokenize(data), 4)\n",
    "print([gram for gram in n_grams])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-skating",
   "metadata": {},
   "source": [
    "#### Using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "little-palmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n",
      "[WordList(['A', 'class']), WordList(['class', 'is']), WordList(['is', 'a']), WordList(['a', 'blueprint']), WordList(['blueprint', 'for']), WordList(['for', 'the']), WordList(['the', 'object'])]\n",
      "[WordList(['A', 'class', 'is', 'a']), WordList(['class', 'is', 'a', 'blueprint']), WordList(['is', 'a', 'blueprint', 'for']), WordList(['a', 'blueprint', 'for', 'the']), WordList(['blueprint', 'for', 'the', 'object'])]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sentence = \"I am super-motivated.\"\n",
    "blob = TextBlob(sentence)\n",
    "blob.tokens\n",
    "blob.tags\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Stop Word Removal\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#There is no functionality that is specific for stopwords removal\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Lemmatizing\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# from textblob lib import Word method \n",
    "from textblob import Word \n",
    "\n",
    "# create a Word object. \n",
    "u = Word(\"rocks\") \n",
    "\n",
    "# apply lemmatization. \n",
    "print(\"rocks :\", u.lemmatize()) \n",
    "\n",
    "# create a Word object. \n",
    "v = Word(\"corpora\") \n",
    "\n",
    "# apply lemmatization. \n",
    "print(\"corpora :\", v.lemmatize()) \n",
    "\n",
    "# create a Word object. \n",
    "w = Word(\"better\") \n",
    "\n",
    "# apply lemmatization with \n",
    "# parameter \"a\", \"a\" denotes adjective. \n",
    "print(\"better :\", w.lemmatize(\"a\")) \n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# N-grams\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#from: https://www.pythonprogramming.in/generate-the-n-grams-for-the-given-sentence-using-nltk-or-textblob.html\n",
    "data = 'A class is a blueprint for the object.'\n",
    "n_grams = TextBlob(data).ngrams(2)\n",
    "print(n_grams)\n",
    "n_grams = TextBlob(data).ngrams(4)\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-prevention",
   "metadata": {},
   "source": [
    "#### Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "incredible-celebrity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "super\n",
      "-\n",
      "motivated\n",
      "\n",
      "\n",
      "I\n",
      "a\n",
      "m\n",
      "super\n",
      "-\n",
      "motivated\n",
      "\n",
      "\n",
      "I \t TOKEN\n",
      "am \t TOKEN\n",
      "super \t TOKEN\n",
      "- \t INFIX\n",
      "motivated \t TOKEN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I am super-motivated\")\n",
    "for token in doc:\n",
    "    print(token)\n",
    "\n",
    "print(\"\\n\")\n",
    "#add special case to specific token\n",
    "special_case = [{ORTH:\"a\"}, {ORTH:\"m\"}]\n",
    "nlp.tokenizer.add_special_case(\"am\", special_case)\n",
    "doc = nlp(\"I am super-motivated\")\n",
    "for token in doc:\n",
    "    print(token)\n",
    "\n",
    "print(\"\\n\")\n",
    "#explaining text with tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "text = '''I am super-motivated'''\n",
    "doc = nlp(text)\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "assert [t.text for t in doc if not t.is_space] == [t[1] for t in tok_exp]\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\t\", t[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-cookie",
   "metadata": {},
   "source": [
    "Spacy has an additional functionality, which is to define your own tokenizer rules. You can use specific functions or base your tokenization on several text parsers, such as regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "prescription-expansion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello-world.', ':)', 'ac[', 'https://www.nltk.org/data.htm', 'l', 'aaaaa', 'c', 'c', 'c', 'c']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "prefix_re = re.compile(r'''ac\\[''')\n",
    "suffix_re = re.compile(r'''c''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, \n",
    "                     prefix_search=prefix_re.search,\n",
    "                     suffix_search=suffix_re.search,\n",
    "                     url_match=simple_url_re.match)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "doc = nlp(\"hello-world. :) ac[https://www.nltk.org/data.html aaaaacccc\")\n",
    "print([t.text for t in doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-failing",
   "metadata": {},
   "source": [
    "There is also a package for tokenizers that can be used on spacy for special cases\n",
    "https://github.com/huggingface/tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "equal-worthy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  ===> a\n",
      "letter  ===> letter\n",
      "has  ===> have\n",
      "been  ===> be\n",
      "written  ===> write\n",
      ",  ===> ,\n",
      "asking  ===> ask\n",
      "him  ===> he\n",
      "to  ===> to\n",
      "be  ===> be\n",
      "released  ===> release\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Stop Word Removal\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import spacy    \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.Defaults.stop_words.add(\"my_new_stopword\")\n",
    "nlp.Defaults.stop_words\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Lemmatizing (The stemmer is from nltk)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/\n",
    "\n",
    "sentence7 = nlp(u'A letter has been written, asking him to be released')\n",
    "\n",
    "for word in sentence7:\n",
    "    print(word.text + '  ===>', word.lemma_)\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# N-grams\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#Spacy does not have a specific Ngram function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-jesus",
   "metadata": {},
   "source": [
    "### Text Parsing - Using Regex\n",
    "\n",
    "Parsing is the process of defining rules for splitting text segments into smaller segments. The definition of these rules follow a specific grammar. One example of such set of rules are regular grammars, a powerful text pattern parsing mechanism from which regular expressions can be defined.\n",
    "\n",
    "Besides regular expressions, text can also be parsed based on other rules. For this, *Spacy*'s library offers a set of rule-based matchers, which include parsing text by means of: (1) text, (2) regular expressions, (3) part-of-speech (POS) tags, (4) lemma, (5) morphological features and (6) dependency tree.\n",
    "\n",
    "In this example, we will show how to use the regex module and the common_regex module, which provides predefined regular expressions for typical text patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "temporal-player",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Matches using re module from Python----------\n",
      "<re.Match object; span=(21, 30), match=' article '>\n",
      "<re.Match object; span=(157, 162), match=' any '>\n",
      "<re.Match object; span=(221, 225), match=' at '>\n",
      "<re.Match object; span=(298, 309), match=' associate '>\n",
      "\n",
      "\n",
      "----- Matches using Common Regex module for time values and emails -------\n",
      "time values:\n",
      "['5:00PM', '4:00']\n",
      "emails:\n",
      "['harold.smith@gmail.com']\n",
      "\n",
      "\n",
      "----- Verbosity -----\n",
      "<re.Match object; span=(312, 334), match='harold.smith@gmail.com'>\n",
      "\n",
      "\n",
      "----- String Replace with Function -----\n",
      "John, please get that article on www.linkedin.com to me by 5:00PM \n",
      "                               on Jan 9th 2012. 4:00 would be ideal, actually. If you have any \n",
      "                               questions, You can reach me at (519)-236-2723x341 or get in touch with\n",
      "                               my associate at *************gmail.com\n",
      "\n",
      "\n",
      "-----Giving tags to the pattern found-----\n",
      "{'email': 'harold.smith@gmail.com'}\n",
      "\n",
      "\n",
      "----- Lookahead -----\n",
      "Search for word that precedes '@'\n",
      "['smith']\n",
      "----- Lookbehind -----\n",
      "Search for word that follows '@'\n",
      "['gmail']\n"
     ]
    }
   ],
   "source": [
    "#Regex usage\n",
    "import re\n",
    "from commonregex import CommonRegex\n",
    "\n",
    "text = \"\"\"John, please get that article on www.linkedin.com to me by 5:00PM \n",
    "                               on Jan 9th 2012. 4:00 would be ideal, actually. If you have any \n",
    "                               questions, You can reach me at (519)-236-2723x341 or get in touch with\n",
    "                               my associate at harold.smith@gmail.com\"\"\"\n",
    "\n",
    "#Use re module from python---------------------------\n",
    "#search for all words that start with an \"a\":\n",
    "search = r\" (a[a-z]+) \"\n",
    "a = re.finditer(search, text)\n",
    "print(\"--------Matches using re module from Python----------\")\n",
    "for i in a:\n",
    "    print(i)\n",
    "\n",
    "#search for specific pattern with CommonRegex--------\n",
    "#search for times in text\n",
    "print(\"\\n\")\n",
    "print(\"----- Matches using Common Regex module for time values and emails -------\")\n",
    "\n",
    "parsed_text = CommonRegex(text)\n",
    "print(\"time values:\")\n",
    "print(parsed_text.times)\n",
    "print(\"emails:\")\n",
    "print(parsed_text.emails)\n",
    "\n",
    "#Additional interests regarding regular expressions\n",
    "#Using Verbosity\n",
    "print(\"\\n\")\n",
    "print(\"----- Verbosity -----\")\n",
    "\n",
    "matches = re.finditer(r'''\n",
    "        \\S+ #Matches any non-whitespace character\n",
    "        @   #Matches @ char\n",
    "        \\S+ #Matches any non-whitespace character\n",
    "''', text, re.VERBOSE)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)\n",
    "\n",
    "#Using Verbosity\n",
    "print(\"\\n\")\n",
    "print(\"----- String Replace with Function -----\")\n",
    "def change_email_name(s):\n",
    "    a = len(re.search(r\"(\\S+)@\", s[0])[0])\n",
    "    return \"*\"*a + s[0][a:]\n",
    "\n",
    "sub = re.sub(\"\\S+@\\S+\", change_email_name, text)\n",
    "print(sub)\n",
    "\n",
    "\n",
    "#Create Dictionnary within the writting pattern:\n",
    "print(\"\\n\")\n",
    "print(\"-----Giving tags to the pattern found-----\")\n",
    "print(re.search(r\"(?P<email>\\S+@\\S+)\", text).groupdict())\n",
    "\n",
    "\n",
    "#Using lookbehinds and Lookaheads:\n",
    "print(\"\\n\")\n",
    "print(\"----- Lookahead -----\")\n",
    "print(\"Search for word that precedes '@'\")\n",
    "print(re.findall(r\"\\w+(?=@)\", text))\n",
    "print(\"----- Lookbehind -----\")\n",
    "print(\"Search for word that follows '@'\")\n",
    "print(re.findall(r\"(?<=@)\\w+\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-technology",
   "metadata": {},
   "source": [
    "### Text Parsing - Using Spacy\n",
    "\n",
    "Spacy has an interesting module to make parsing assumptions and multiple levels of parsing methods. The following link shows all the rules available to make parsing:\n",
    "\n",
    "https://spacy.io/usage/rule-based-matching#matcher\n",
    "\n",
    "With this method we are able to define multiple rules that define the pattern. This gives a lot of freedom to perform a search on the text space.\n",
    "\n",
    "#### Entity, Shape, Tag, Lemma Rules\n",
    "\n",
    "Rules can be made with multiple levels of word analysis. For instance, words can be searched based on the fact that these are verbs, or pronouns, or even if their shape is *xx*. Here is an example of the table used as a reference:\n",
    "\n",
    "![Caption](../Figures/OverallRulesSpacy.PNG)\n",
    "\n",
    "#### Dependency Rules\n",
    "\n",
    "Rules to implement parsing based on the dependencies between words in a sentence. Operators for tree dependency\n",
    "\n",
    "![Caption](../Figures/DependencyRulesSpacy.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "diagnostic-range",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- POS MATCHER -----------\n",
      "\n",
      "\n",
      "pattern:\n",
      "pattern1 = [{\"POS\": \"NOUN\", \"TEXT\":{\"REGEX\":\"^h\\w+\"}}]\n",
      "healthcare\n",
      "----------- DEPENDENCY MATCHER -----------\n",
      "\n",
      "\n",
      "pattern:\n",
      "pattern = [\n",
      "  {\n",
      "    \"RIGHT_ID\": \"anchor_founded\",       # unique name\n",
      "    \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}  # token pattern for \"founded\"\n",
      "  },\n",
      "  {\n",
      "    \"LEFT_ID\": \"anchor_founded\",\n",
      "    \"REL_OP\": \">\", #leftid is the head of the parsing tree, which means, what comes next to anchor is matched\n",
      "    \"RIGHT_ID\": \"founded_subject\",\n",
      "    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}, #the dependency between the anchor and this has to be nsubj \n",
      "   },\n",
      "   {\n",
      "    \"LEFT_ID\": \"anchor_founded\",\n",
      "    \"REL_OP\": \">\", #leftid is the head of the parsing tree, which means, what comes next to anchor is matched\n",
      "    \"RIGHT_ID\": \"founded_prep\",\n",
      "    \"RIGHT_ATTRS\": {\"DEP\": \"prep\"}, #the dependency between the anchor and this has to be prep \n",
      "   }\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"2d17e07a2de4415fb51c074b74372ecc-0\" class=\"displacy\" width=\"1275\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Smith</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">founded</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">healthcare</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">company</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">2005.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2d17e07a2de4415fb51c074b74372ecc-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2d17e07a2de4415fb51c074b74372ecc-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2d17e07a2de4415fb51c074b74372ecc-0-1\" stroke-width=\"2px\" d=\"M420,352.0 C420,177.0 740.0,177.0 740.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2d17e07a2de4415fb51c074b74372ecc-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2d17e07a2de4415fb51c074b74372ecc-0-2\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2d17e07a2de4415fb51c074b74372ecc-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2d17e07a2de4415fb51c074b74372ecc-0-3\" stroke-width=\"2px\" d=\"M245,352.0 C245,89.5 745.0,89.5 745.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2d17e07a2de4415fb51c074b74372ecc-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,354.0 L753.0,342.0 737.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2d17e07a2de4415fb51c074b74372ecc-0-4\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 925.0,2.0 925.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2d17e07a2de4415fb51c074b74372ecc-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,354.0 L933.0,342.0 917.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2d17e07a2de4415fb51c074b74372ecc-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2d17e07a2de4415fb51c074b74372ecc-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match1:\n",
      "founded\n",
      "match0:\n",
      "Smith\n",
      "match5:\n",
      "in\n"
     ]
    }
   ],
   "source": [
    "#collect data from matches\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]  # Matched span\n",
    "    sent = span.sent  # Sentence containing matched span\n",
    "    # Append mock entity for match in displaCy style to matched_sents\n",
    "    # get the match span by ofsetting the start and end of the span with the\n",
    "    # start and end of the sentence in the doc\n",
    "    match_ents = [{\n",
    "        \"start\": span.start_char - sent.start_char,\n",
    "        \"end\": span.end_char - sent.start_char,\n",
    "        \"label\": \"MATCH\",\n",
    "    }]\n",
    "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import DependencyMatcher, Matcher\n",
    "\n",
    "text = \"Smith founded a healthcare company in 2005.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "print(\"----------- POS MATCHER -----------\")\n",
    "print(\"\\n\")\n",
    "print(\"pattern:\")\n",
    "print(\"\"\"pattern1 = [{\"POS\": \"NOUN\", \"TEXT\":{\"REGEX\":\"^h\\w+\"}}]\"\"\")\n",
    "matched_sents = []\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"POS\": \"NOUN\", \"TEXT\":{\"REGEX\":\"^h\\w+\"}}]\n",
    "\n",
    "matcher.add(\"multiple\", [pattern1], on_match=collect_sents)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match in matched_sents:\n",
    "    i = match[\"ents\"][0]\n",
    "    print(text[i[\"start\"]:i[\"end\"]])\n",
    "\n",
    "\n",
    "    \n",
    "print(\"----------- DEPENDENCY MATCHER -----------\")\n",
    "print(\"\\n\")\n",
    "print(\"pattern:\")\n",
    "print(\"\"\"pattern = [\n",
    "  {\n",
    "    \"RIGHT_ID\": \"anchor_founded\",       # unique name\n",
    "    \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}  # token pattern for \"founded\"\n",
    "  },\n",
    "  {\n",
    "    \"LEFT_ID\": \"anchor_founded\",\n",
    "    \"REL_OP\": \">\", #leftid is the head of the parsing tree, which means, what comes next to anchor is matched\n",
    "    \"RIGHT_ID\": \"founded_subject\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}, #the dependency between the anchor and this has to be nsubj \n",
    "   },\n",
    "   {\n",
    "    \"LEFT_ID\": \"anchor_founded\",\n",
    "    \"REL_OP\": \">\", #leftid is the head of the parsing tree, which means, what comes next to anchor is matched\n",
    "    \"RIGHT_ID\": \"founded_prep\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": \"prep\"}, #the dependency between the anchor and this has to be prep \n",
    "   }\n",
    "]\"\"\")\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "pattern = [\n",
    "  {\n",
    "    \"RIGHT_ID\": \"anchor_founded\",       # unique name\n",
    "    \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}  # token pattern for \"founded\"\n",
    "  },\n",
    "  {\n",
    "    \"LEFT_ID\": \"anchor_founded\",\n",
    "    \"REL_OP\": \">\", #leftid is the head of the parsing tree, which means, what comes next to anchor is matched\n",
    "    \"RIGHT_ID\": \"founded_subject\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}, #the dependency between the anchor and this has to be nsubj \n",
    "   },\n",
    "   {\n",
    "    \"LEFT_ID\": \"anchor_founded\",\n",
    "    \"REL_OP\": \">\", #leftid is the head of the parsing tree, which means, what comes next to anchor is matched\n",
    "    \"RIGHT_ID\": \"founded_prep\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": \"prep\"}, #the dependency between the anchor and this has to be prep \n",
    "   }\n",
    "]\n",
    "\n",
    "matcher.add(\"FOUNDED\", [pattern])\n",
    "displacy.render(doc, jupyter=True)\n",
    "matches = matcher(doc)\n",
    "\n",
    "for i in matches[0][1]:\n",
    "    print(\"match\"+str(i)+\":\")\n",
    "    print(doc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-chorus",
   "metadata": {},
   "source": [
    "### Text Parsing - Fuzzy text parsing (TODO)\n",
    "\n",
    "Sometimes, text parsing can be made with a certain level of difference. Imagine you want to find a word \"made\", but by mistake, words such as \"maide\" were written...sometimes, fuzzyness in the search is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "informational-northern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Fuzzy Matcher -----------\n",
      "\n",
      "\n",
      "pattern:\n",
      "[]\n",
      "----------- Regex Fuzzy Matcher -----------\n",
      "\n",
      "\n",
      "pattern:\n",
      "----------- Similarity MATCHER -----------\n",
      "\n",
      "\n",
      "pattern:\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\biosignals3\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\spaczz\\search\\similaritysearcher.py:56: MissingVectorsWarning: The spaCy Vocab object has no word vectors.\n",
      "                Similarity results may not be useful.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spaczz.matcher import FuzzyMatcher, RegexMatcher, SimilarityMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Smiths founded a healthcare company in 2005.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"----------- Fuzzy Matcher -----------\")\n",
    "print(\"\\n\")\n",
    "print(\"pattern:\")\n",
    "\n",
    "def add_name_ent(matcher, doc, i, matches):\n",
    "    \"\"\"Callback on match function. Adds \"NAME\" entities to doc.\"\"\"\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    _match_id, start, end, _ratio = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"NAME\")\n",
    "    doc.ents += (entity,)\n",
    "\n",
    "matcher = RegexMatcher(nlp.vocab)\n",
    "matcher.add(\"NAME\", [r\"(Smith)\"], on_match=add_name_ent)\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "\n",
    "print(\"----------- Regex Fuzzy Matcher -----------\")\n",
    "print(\"\\n\")\n",
    "print(\"pattern:\")\n",
    "\n",
    "\n",
    "print(\"----------- Similarity MATCHER -----------\")\n",
    "print(\"\\n\")\n",
    "print(\"pattern:\")\n",
    "\n",
    "# lowering min_r2 from default of 75 to produce matches in this example\n",
    "matcher = SimilarityMatcher(nlp.vocab, thresh=0.5)\n",
    "matcher.add(\"name\", [nlp(\"health\")])\n",
    "matches = matcher(doc)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-amsterdam",
   "metadata": {},
   "source": [
    "## Grammar Inference (TODO)\n",
    "\n",
    "Besides text-parsing methods based on grammars, from which text is parsed following a specific criteria, there are also grammar inference methods, which are used to infer the rules and therefore, the structure and language of a piece of text. Examples of such algorithms are the Sequitur-algorithm, which infers context-free-grammars and Regular in Positive and Negative Inference (RPNI), which infers regular grammars. For this example, we will use the Sequitur algorithm and the RPNI for regular inference (from: https://github.com/steynvl/inferrer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "sharp-resistance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 1 1 2\n",
      "1 -> 2 _                                          abc_\n",
      "2 -> a b c                                        abc\n",
      "[Production(1), Production(1), Production(2)]\n",
      "[Production(2), ' ']\n",
      "['a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "from sksequitur import parse, Production\n",
    "\n",
    "text = \"abc abc abc\"\n",
    "\n",
    "#Parse text\n",
    "grammar1 = parse(text)\n",
    "print(grammar)\n",
    "print(grammar[0])\n",
    "print(grammar[1])\n",
    "print(grammar[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "proper-advance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ba*ba*b|a)*(ba*|())\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# import argparse\n",
    "# from methods import inferrer\n",
    "# from typing import Set\n",
    "\n",
    "# pos_examples = set([\"abababab\", \"abababab\"])\n",
    "# neg_examples = set([\"abab\"])\n",
    "# alphabet = mt.inferrer.utils.determine_alphabet(pos_examples.union(neg_examples))\n",
    "\n",
    "# algorithm = \"rpni\"\n",
    "\n",
    "# if algorithm in ['rpni', 'gold']:\n",
    "#     learner = inferrer.Learner(alphabet=alphabet,\n",
    "#                                pos_examples=pos_examples,\n",
    "#                                neg_examples=neg_examples,\n",
    "#                                algorithm=algorithm)\n",
    "# elif algorithm in ['lstar', 'nlstar']:\n",
    "#     learner = inferrer.Learner(alphabet=alphabet,\n",
    "#                                oracle=inferrer.oracle.PassiveOracle(pos_examples,\n",
    "#                                                                     neg_examples),\n",
    "#                                algorithm=algorithm)\n",
    "\n",
    "# dfa = learner.learn_grammar()\n",
    "# print(dfa.to_regex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-karen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
