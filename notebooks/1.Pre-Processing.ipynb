{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "technological-satellite",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In this notebook, the main pre-processing strategies for text are exemplified with tools such as nltk, spacy and textblob.\n",
    "These steps involve: \n",
    "\n",
    "- tokenization;\n",
    "- stop word removal;\n",
    "- lemmatization/stemming;\n",
    "- N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-general",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization refers to the process of segmenting text in tokens. These may represent words, characters or subwords. For instance, the sentence \"I am supermotivated\" can be separated in \"[I, am, supermotivated]\" or \"[I, am, super, motivated]\". This step is very relevant to prepare the data for next processing steps. \n",
    "\n",
    "### Stop Words Removal\n",
    "\n",
    "Stop words are referred as words that are not important for processing. In fact, these words are defined as noise and can be prejudicial to the analysis. For this reason, a list of stop words exist for each language, being removed from text before next processing steps. Typically these are very common words, which might confuse the system in evaluating the similarity between documents.\n",
    "\n",
    "### Lemmatizing/Stemming\n",
    "\n",
    "As mentioned, languages use conjugation for gender, verbs, etc...which requires making variations of a word, and in some cases, with irregular conjugations, changing the root form of the word completely. Having the root form of a word is better for post-processing methodologies, since words conjugated differently will be treated as the same. Considering the sentence \"I \\textbf{like} to have something that he \\textbf{likes}\", \"like\" and \"likes\" are originated from the same root word \"like\", and should be considered as \"like\" for the benefit of improving the analysis of some methodologies. There are two ways of simplifying this text representation: Lemmatisation and Stemming.\n",
    "\n",
    " Lemmatisation is the process of grouping together the inflected forms of a word considering the lemma, that is the dictionary form (e.g: \"\\textit{to walk}\", \"\\textit{walked}\" or \"\\textit{walking}\" have the same lemma \"\\textit{walk}\".\n",
    "\n",
    " Stemming differs from Lemmatisation in the sense that the meaning is not inferred as in the case of the lemma. A stemma is the root form of a word, e.g. \"\\textit{world}\" is the stem of \"\\textit{worldwide}\" and \"\\textit{worlds}\". \n",
    " \n",
    "### N-grams\n",
    "\n",
    "Text is a sequential structure, in which the \\textit{next} word has a certain level of dependency from \\textit{previous} ones. Therefore, when designing probabilistic models, having the text structured in a way that we are able to understand these dependencies is of great relevance. In that sense, the N-grams structure was designed for this purpose. N-grams are a structured way of organizing the text by grouping \\textit{N} tokens that are followed, with a total overlap of the sequence. For instance, the sentence \"I am doing great\", organized as a bigram (2-grams), would be: [\"I am\", \"am doing\", \"doing great\"]. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-establishment",
   "metadata": {},
   "source": [
    "#### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "instrumental-resolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sentence:------------\n",
      "['I', 'am', 'super-motivated', '.']\n",
      "\n",
      "tagged tokens:------------\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('super-motivated', 'JJ'), ('.', '.')]\n",
      "\n",
      "named entities:------------\n",
      "(S I/PRP am/VBP super-motivated/JJ ./.)\n",
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n",
      "studi\n",
      "studi\n",
      "cri\n",
      "cri\n",
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n",
      "[('A', 'class'), ('class', 'is'), ('is', 'a'), ('a', 'blueprint'), ('blueprint', 'for'), ('for', 'the'), ('the', 'object'), ('object', '.')]\n",
      "[('A', 'class', 'is', 'a'), ('class', 'is', 'a', 'blueprint'), ('is', 'a', 'blueprint', 'for'), ('a', 'blueprint', 'for', 'the'), ('blueprint', 'for', 'the', 'object'), ('for', 'the', 'object', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk as nltk\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Tokenization\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "sentence = \"I am super-motivated.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(\"tokenized sentence:------------\")\n",
    "print(tokens)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(\"\\ntagged tokens:------------\")\n",
    "print(tagged)\n",
    "\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(\"\\nnamed entities:------------\")\n",
    "print(entities)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Stop Word Removal\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#from: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example_sent = \"\"\"This is a sample sentence, showing off the stop words filtration.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example_sent) \n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "\n",
    "filtered_sentence = [] \n",
    "\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "print(word_tokens) \n",
    "print(filtered_sentence) \n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Lemmatizing\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#from: https://www.guru99.com/stemming-lemmatization-python-nltk.html\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "e_words= [\"studies\", \"studying\", \"cries\", \"cry\"]\n",
    "\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)\n",
    "    \n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))\n",
    "    \n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# N-grams\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#from: https://www.pythonprogramming.in/generate-the-n-grams-for-the-given-sentence-using-nltk-or-textblob.html\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "data = 'A class is a blueprint for the object.'\n",
    "\n",
    "n_grams = ngrams(nltk.word_tokenize(data), 2)\n",
    "print([gram for gram in n_grams])\n",
    "n_grams = ngrams(nltk.word_tokenize(data), 4)\n",
    "print([gram for gram in n_grams])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-skating",
   "metadata": {},
   "source": [
    "#### Using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "little-palmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n",
      "[WordList(['A', 'class']), WordList(['class', 'is']), WordList(['is', 'a']), WordList(['a', 'blueprint']), WordList(['blueprint', 'for']), WordList(['for', 'the']), WordList(['the', 'object'])]\n",
      "[WordList(['A', 'class', 'is', 'a']), WordList(['class', 'is', 'a', 'blueprint']), WordList(['is', 'a', 'blueprint', 'for']), WordList(['a', 'blueprint', 'for', 'the']), WordList(['blueprint', 'for', 'the', 'object'])]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sentence = \"I am super-motivated.\"\n",
    "blob = TextBlob(sentence)\n",
    "blob.tokens\n",
    "blob.tags\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Stop Word Removal\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#There is no functionality that is specific for stopwords removal\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Lemmatizing\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# from textblob lib import Word method \n",
    "from textblob import Word \n",
    "\n",
    "# create a Word object. \n",
    "u = Word(\"rocks\") \n",
    "\n",
    "# apply lemmatization. \n",
    "print(\"rocks :\", u.lemmatize()) \n",
    "\n",
    "# create a Word object. \n",
    "v = Word(\"corpora\") \n",
    "\n",
    "# apply lemmatization. \n",
    "print(\"corpora :\", v.lemmatize()) \n",
    "\n",
    "# create a Word object. \n",
    "w = Word(\"better\") \n",
    "\n",
    "# apply lemmatization with \n",
    "# parameter \"a\", \"a\" denotes adjective. \n",
    "print(\"better :\", w.lemmatize(\"a\")) \n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# N-grams\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#from: https://www.pythonprogramming.in/generate-the-n-grams-for-the-given-sentence-using-nltk-or-textblob.html\n",
    "data = 'A class is a blueprint for the object.'\n",
    "n_grams = TextBlob(data).ngrams(2)\n",
    "print(n_grams)\n",
    "n_grams = TextBlob(data).ngrams(4)\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-prevention",
   "metadata": {},
   "source": [
    "#### Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "incredible-celebrity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "super\n",
      "-\n",
      "motivated\n",
      "\n",
      "\n",
      "I\n",
      "a\n",
      "m\n",
      "super\n",
      "-\n",
      "motivated\n",
      "\n",
      "\n",
      "I \t TOKEN\n",
      "am \t TOKEN\n",
      "super \t TOKEN\n",
      "- \t INFIX\n",
      "motivated \t TOKEN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I am super-motivated\")\n",
    "for token in doc:\n",
    "    print(token)\n",
    "\n",
    "print(\"\\n\")\n",
    "#add special case to specific token\n",
    "special_case = [{ORTH:\"a\"}, {ORTH:\"m\"}]\n",
    "nlp.tokenizer.add_special_case(\"am\", special_case)\n",
    "doc = nlp(\"I am super-motivated\")\n",
    "for token in doc:\n",
    "    print(token)\n",
    "\n",
    "print(\"\\n\")\n",
    "#explaining text with tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "text = '''I am super-motivated'''\n",
    "doc = nlp(text)\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "assert [t.text for t in doc if not t.is_space] == [t[1] for t in tok_exp]\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\t\", t[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-cookie",
   "metadata": {},
   "source": [
    "Spacy has an additional functionality, which is to define your own tokenizer rules. You can use specific functions or base your tokenization on several text parsers, such as regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "prescription-expansion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello-world.', ':)', 'ac[', 'https://www.nltk.org/data.htm', 'l', 'aaaaa', 'c', 'c', 'c', 'c']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "prefix_re = re.compile(r'''ac\\[''')\n",
    "suffix_re = re.compile(r'''c''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, \n",
    "                     prefix_search=prefix_re.search,\n",
    "                     suffix_search=suffix_re.search,\n",
    "                     url_match=simple_url_re.match)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "doc = nlp(\"hello-world. :) ac[https://www.nltk.org/data.html aaaaacccc\")\n",
    "print([t.text for t in doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-failing",
   "metadata": {},
   "source": [
    "There is also a package for tokenizers that can be used on spacy for special cases\n",
    "https://github.com/huggingface/tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "equal-worthy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  ===> a\n",
      "letter  ===> letter\n",
      "has  ===> have\n",
      "been  ===> be\n",
      "written  ===> write\n",
      ",  ===> ,\n",
      "asking  ===> ask\n",
      "him  ===> he\n",
      "to  ===> to\n",
      "be  ===> be\n",
      "released  ===> release\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Stop Word Removal\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import spacy    \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.Defaults.stop_words.add(\"my_new_stopword\")\n",
    "nlp.Defaults.stop_words\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Lemmatizing (The stemmer is from nltk)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/\n",
    "\n",
    "sentence7 = nlp(u'A letter has been written, asking him to be released')\n",
    "\n",
    "for word in sentence7:\n",
    "    print(word.text + '  ===>', word.lemma_)\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# N-grams\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#Spacy does not have a specific Ngram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-player",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
