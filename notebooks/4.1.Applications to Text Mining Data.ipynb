{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "african-retrieval",
   "metadata": {},
   "source": [
    "# NLP Applications for Text Mining\n",
    "\n",
    "In this notebook will be showed how to use NLP techniques with Python tools for several examples, following this list:\n",
    "\n",
    "- Part of Speech Tagging (POS)\n",
    "- Named Entity Recognition (NER)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-block",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging\n",
    "\n",
    "Part of Speech Tagging (POS) is the process of identifying the part to which a word belongs in a sentence of a corpus, for instance, if it is a verb, subject or a pronoun. We will use the Spacy framework to show you examples of its usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "architectural-stuart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"8c9eeeddb55547bba59903e17d2e1e52-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">perform</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">tagging</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">words</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8c9eeeddb55547bba59903e17d2e1e52-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8c9eeeddb55547bba59903e17d2e1e52-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8c9eeeddb55547bba59903e17d2e1e52-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8c9eeeddb55547bba59903e17d2e1e52-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8c9eeeddb55547bba59903e17d2e1e52-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8c9eeeddb55547bba59903e17d2e1e52-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8c9eeeddb55547bba59903e17d2e1e52-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8c9eeeddb55547bba59903e17d2e1e52-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8c9eeeddb55547bba59903e17d2e1e52-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8c9eeeddb55547bba59903e17d2e1e52-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I perform the tagging of words\")\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "native-fields",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"451e015e8fa34f57a580f23e15984a91-0\" class=\"displacy\" width=\"650\" height=\"237.0\" direction=\"ltr\" style=\"max-width: none; height: 237.0px; color: #2C9CD8; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">perform</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">VBP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">DT</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">tagging</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">NN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">IN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">words</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">NNS</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-451e015e8fa34f57a580f23e15984a91-0-0\" stroke-width=\"2px\" d=\"M70,102.0 C70,52.0 145.0,52.0 145.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-451e015e8fa34f57a580f23e15984a91-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,104.0 L62,92.0 78,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-451e015e8fa34f57a580f23e15984a91-0-1\" stroke-width=\"2px\" d=\"M270,102.0 C270,52.0 345.0,52.0 345.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-451e015e8fa34f57a580f23e15984a91-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M270,104.0 L262,92.0 278,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-451e015e8fa34f57a580f23e15984a91-0-2\" stroke-width=\"2px\" d=\"M170,102.0 C170,2.0 350.0,2.0 350.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-451e015e8fa34f57a580f23e15984a91-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M350.0,104.0 L358.0,92.0 342.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-451e015e8fa34f57a580f23e15984a91-0-3\" stroke-width=\"2px\" d=\"M370,102.0 C370,52.0 445.0,52.0 445.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-451e015e8fa34f57a580f23e15984a91-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M445.0,104.0 L453.0,92.0 437.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-451e015e8fa34f57a580f23e15984a91-0-4\" stroke-width=\"2px\" d=\"M470,102.0 C470,52.0 545.0,52.0 545.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-451e015e8fa34f57a580f23e15984a91-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M545.0,104.0 L553.0,92.0 537.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options = {\"fine_grained\":True, \"distance\":100, \"color\":\"#2C9CD8\"}\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-bermuda",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "changed-milton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GOOGLE\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " has developed a new algorithm to surpass \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    IBM\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". </div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John Bugoiggy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is responsible for developing this strategy, which is believed to be the best of \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the 21st century\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"GOOGLE has developed a new algorithm to surpass IBM. John Bugoiggy is responsible for developing this strategy, which is believed to be the best of the 21st century.\"\"\"\n",
    "doc = nlp(text)\n",
    "sentence_spans = list(doc.sents)\n",
    "displacy.render(sentence_spans, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-tragedy",
   "metadata": {},
   "source": [
    "## Text Summarization\n",
    "\n",
    "Techniques for text summarization are divided into a) ***abstractive\n",
    "based*** and b) ***extractive based***. Method **a** requires the usage\n",
    "of a more complex process, involving the inference of word and sentence\n",
    "relationships and developing a sentence generator based on the syntactic\n",
    "relationships found. This strategy is supervised and is computationally\n",
    "expensive. In the other end, method **b** is unsupervised and provides a\n",
    "summarization of text by means of ranking the importance of sentences.\n",
    "This importance level is calculated with similarity measures from\n",
    "token/word frequency matrices, such as *BoW* or *TFIDF*. The steps for extractive methods are (1) Compute a normalized term frequency matrix and the total frequency of words in the document; (2) compute the sentence relevance by means of similarity measures or by calculating the sum of the word normalized frequencies that belong to each sentence and (3) sort the sentence by importance level. The summary of the document is then the set of more important sentences.\n",
    "\n",
    "\n",
    "![Caption](../Figures/TextSummarize.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-designation",
   "metadata": {},
   "source": [
    "#### Text Summarization with Gensim\n",
    "(Example taken from: [George Pipis](https://python-bloggers.com/2020/09/text-summarization-in-python-with-gensim/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "initial-alias",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The LDA process generates documents based on a probability modulated by the number of topics and words present in documents.', 'The LDA typically accepts a \\textit{BoW} model, from which it derives the topics to which documents are associated based on the words that these contain.']\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "\n",
    "text = \"\"\"The Latent Dirichlet Allocation approach is an unsupervised method used for Topic Modelling that is supported in two main considerations: (1) Documents that have similar words must have similar topics; (2) Documents that have groups of words frequently occurring together usually have the same topic. LDA is a fuzzy clustering method that provides a value that reflects the membership of the sentence into a specific topic. The clustering process is performed based on the adequacy of a sentence to a generated corpus. \n",
    "The LDA process generates documents based on a probability modulated by the number of topics and words present in documents. This probability has the following equation:\n",
    "In the first case, the Dirichlet distribution evaluates the probability of a document belonging to a specific Topic, while the second evaluates the probability of a topic being associated with a word. The triangles can help visualize these distributions. The blue triangle has topics as edges, while documents as elements. The closer the words are from the edges, the closer these are related to that topic. In the other hand, the yellow tetrahedron has words as edges, while topics as elements. The $\\theta$ and $\\phi$ parameters are associated with multinomial distributions corresponding to topics and words, respectively. From these distributions, words and topics are generated and combined to generate a document. The documents are generated based on the arrangement between document-topics probabilities and topic-words probabilities. The generated document that maximizes the probability and best fit the inputted document will be classified based on the arrangement, and be assigned a membership probability to each topic.\n",
    "The LDA typically accepts a \\textit{BoW} model, from which it derives the topics to which documents are associated based on the words that these contain. \\textbf{Examples of applying LDA can be seen in the following \\textit{notebook}}.\"\"\"\n",
    "\n",
    "test_summary = summarize(text, ratio=0.15, split=True)\n",
    "\n",
    "print(test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-earth",
   "metadata": {},
   "source": [
    "#### Text Summarization with Spacy\n",
    "(Example taken from [Jesse E. Agbe](https://github.com/Jcharis/Natural-Language-Processing-Tutorials/blob/master/NLP_with_SpaCy/Text%20Summarization%20In%20SpaCy.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "finished-carbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated document that maximizes the probability and best fit the inputted document will be classified based on the arrangement, and be assigned a membership probability to each topic. The LDA typically accepts a \textit{BoW} model, from which it derives the topics to which documents are associated based on the words that these contain. The closer the words are from the edges, the closer these are related to that topic.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Build an NLP Object\n",
    "docx = nlp(text)\n",
    "# Tokenization of Text\n",
    "mytokens = [token.text for token in docx]\n",
    "#Word Frequency table\n",
    "# Build Word Frequency\n",
    "# word.text is tokenization in spacy\n",
    "word_frequencies = {}\n",
    "for word in docx:\n",
    "        if word.text not in word_frequencies.keys():\n",
    "            word_frequencies[word.text] = 1\n",
    "        else:\n",
    "            word_frequencies[word.text] += 1\n",
    "\n",
    "#Normalize \n",
    "maximum_frequency = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():  \n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
    "\n",
    "        #sentence list\n",
    "sentence_list = [ sentence for sentence in docx.sents ]\n",
    "[w.text.lower() for t in sentence_list for w in t ]\n",
    "\n",
    "#ranking sentences:\n",
    "sentence_scores = {}  \n",
    "for sent in sentence_list:  \n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if len(sent.text.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "\n",
    "sorted_sentences = [sentence[0] for sentence in sorted(sentence_scores.items(), key=lambda value: value[1])[::-1]]\n",
    "\n",
    "#Only print the first 3 sentences\n",
    "final_sentences = [ w.text for w in sorted_sentences[:3]]\n",
    "summary = ' '.join(final_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-lotus",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "Classification of text documents is a very common task in NLP. The purpose is to assign classes to documents based on their content. Examples of this type of problem are seen in many of our daily tasks. For instance, web search engines use topic modelling to assign the best documents based on the queries inputted, or systems that provide content-based recommendations, such as streaming services, use topic modelling to match the best content to your content history. All these tasks are related to document classification.\n",
    "\n",
    "![Caption](Figures/TopicModelling.PNG)\n",
    "\n",
    "\n",
    "The process to compute document classification is presented in the figure and involves using one of the *Text Processing* methods discussed in the previous section with a *BoW* or *TFIDF* matrix. These methods are *LSI*, *LDA* and *NMF*. The result depends on the method used, but typically we are able to have the main topics and keywords that describe the content of each document, and finally, based on the topics extracted, classify the document as being majorly associated with a specific topic. \n",
    "\n",
    "In this example, we will use sklearn and gensim to test several topic modelling strategies. We will use 2 datasets (Atheism and Graphics). We will test these with LDA, LSI and NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "saved-vertex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Loading a dataset\n",
    "categories = ['alt.atheism', \"comp.graphics\"]\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train_dataset = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=42)\n",
    "train_dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "vanilla-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute BoW or TFIDF\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(train_dataset.data)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=\"english\", max_features=1000)\n",
    "tfidf_matrix = tfidf_vect.fit_transform(train_dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-liberal",
   "metadata": {},
   "source": [
    "#### Latent Dirichelet Allocation (LDA) with BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "married-recall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#Start LDA transformer\n",
    "LDA = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "LDA.fit(doc_term_matrix)\n",
    "\n",
    "#fit and transform matrix\n",
    "topic_values = LDA.transform(doc_term_matrix)\n",
    "docs = 25\n",
    "labels = range(docs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(labels, topic_values[:docs,0], 1, alpha=0.75)\n",
    "ax.bar(labels, topic_values[:docs,1], bottom=topic_values[:docs,0], alpha=0.75)\n",
    "# ax.bar(labels, topic_values[:docs,2], bottom=topic_values[:docs,1], alpha=0.75)\n",
    "print(train_dataset.target[:docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-continent",
   "metadata": {},
   "source": [
    "#### Latent Dirichelet Allocation (LDA) with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "outdoor-surrey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Class for Document\n",
      "[0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Start LDA transformer\n",
    "LDA = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "LDA.fit(tfidf_matrix)\n",
    "\n",
    "#fit and transform matrix\n",
    "topic_values = LDA.transform(tfidf_matrix)\n",
    "docs = 25\n",
    "labels = range(docs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(labels, topic_values[:docs,0], 1, alpha=0.75)\n",
    "ax.bar(labels, topic_values[:docs,1], bottom=topic_values[:docs,0], alpha=0.75)\n",
    "# ax.bar(labels, topic_values[:docs,2], bottom=topic_values[:docs,1], alpha=0.75)\n",
    "print(\"Target Class for Document\")\n",
    "print(train_dataset.target[:docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-worthy",
   "metadata": {},
   "source": [
    "#### Non Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "forty-withdrawal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\biosignals3\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:\n",
      "[0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0]\n",
      "Predicted:\n",
      "[1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "n_components = 2\n",
    "\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss=\"kullback-leibler\", solver=\"mu\", max_iter=1000, alpha=.1, l1_ratio=.5).fit(tfidf_matrix)\n",
    "\n",
    "topic_values = nmf.transform(tfidf_matrix)\n",
    "pred = topic_values.argmax(axis=1)\n",
    "\n",
    "print(\"Target:\")\n",
    "print(train_dataset.target[:docs])\n",
    "print(\"Predicted:\")\n",
    "print(pred[:docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-municipality",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "beneficial-corner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(15313 unique tokens: ['125245', '12872', '1993apr15', '1qie61', '2000']...)\n",
      "Targeted:\n",
      "[0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0]\n",
      "Predicted:\n",
      "[0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from gensim import models\n",
    "from gensim import corpora\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "documents = train_dataset.data\n",
    "eng_stopwords = set(nlp.Defaults.stop_words)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "stemmer = PorterStemmer()\n",
    "translate_tab = {ord(p): u\" \" for p in punctuation}\n",
    "\n",
    "def text2tokens(raw_text):\n",
    "    \"\"\"Split the raw_text string into a list of stemmed tokens.\"\"\"\n",
    "    clean_text = raw_text.lower().translate(translate_tab)\n",
    "    tokens = [token.strip() for token in tokenizer.tokenize(clean_text)]\n",
    "    tokens = [token for token in tokens if token not in eng_stopwords]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return [token for token in stemmed_tokens if len(token) > 2]  # skip short tokens\n",
    "\n",
    "\n",
    "dataset = [text2tokens(txt) for txt in documents]  # convert a documents to list of tokens\n",
    "\n",
    "dictionary = corpora.Dictionary(dataset)\n",
    "print(dictionary)\n",
    "corpus = [dictionary.doc2bow(text) for text in dataset]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)  # step 1 -- initialize a model\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)  # initialize an LSI transformation\n",
    "corpus_lsi = lsi_model[corpus_tfidf] \n",
    "\n",
    "# From the lsi_model we get new vectors\n",
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(corpus_lsi)\n",
    "self_sims = index[corpus_lsi]\n",
    "\n",
    "#From LSI we converted the TFIDF to LSI vector space. Now, to get the classification, we should cluster the similarity matrix\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(self_sims)\n",
    "pred = kmeans.labels_\n",
    "\n",
    "print(\"Targeted:\")\n",
    "print(train_dataset.target[:25])\n",
    "print(\"Predicted:\")\n",
    "print(pred[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-candidate",
   "metadata": {},
   "source": [
    "### Main Conclusion\n",
    "\n",
    "- LDA works better than the remaining methods\n",
    "- NMF did not perform well\n",
    "- LSI works well based on the distance of vectors\n",
    "\n",
    "- LDA works better with BoW\n",
    "- LSI works better with TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-identifier",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "The analysis of sentiment in text corresponds to identifying which type of emotion is provided in the text content, typically separated in two main classes: positive and negative sentiment. This analysis is very common nowadays, specially in the context of online reviews, comments on social networks, etc... In order to apply this analysis in text, we can use pre-trained models from **textblob** or train a classifier with pre-labelled data. This notebook will show these two ways of making Sentiment Analysis. The examples were taken from: \n",
    "- https://kgptalkie.com/amazon-and-imdb-review-sentiment-classification-using-spacy/\n",
    "- https://stackabuse.com/sentiment-analysis-in-python-with-textblob/\n",
    "\n",
    "![Caption](Figures\\SentimentAnalysis.PNG)\n",
    "\n",
    "The process involves (1) pre-processing the textual data and (2) training a classifier with labelled data. The pre-processing step includes the cleaning of text, by removing and adjusting words to a format that promotes better classification, namely by removing stop-words, lemmatizing and, in additionally to the typical pre-processing tasks, use only words associated with emotions, such as verbs and adjectives. This step can be made with a *POS* pre-trained model. From the extracted tokens, create a *BoW* or *TFIDF* matrix, by sentence or paragraph. Each sentence will be associated with a specific sentiment class. The model can then be trained with the *BoW* or *TFIDF* matrix, and used afterwards to classify new sentences and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "adjusted-lloyd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.5, subjectivity=0.26666666666666666)\n"
     ]
    }
   ],
   "source": [
    "# Importing TextBlob\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# Preparing an input sentence\n",
    "sentence = '''The platform provides universal access to the world's best education, partnering with top universities and organizations to offer courses online.'''\n",
    "\n",
    "# Creating a textblob object and assigning the sentiment property\n",
    "analysis = TextBlob(sentence).sentiment\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "monetary-trader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0\n",
       "2  Attempting artiness with black & white and cle...          0\n",
       "3       Very little music or anything to speak of.            0\n",
       "4  The best scene in the movie was when Gerardo i...          1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load IMDB data\n",
    "columns_name = ['Review', 'Sentiment']\n",
    "data_imdb = pd.read_csv('Data/imdb_labelled.txt', sep = '\\t', header = None)\n",
    "data_imdb.columns = columns_name\n",
    "data_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "compact-stock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7647058823529411\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.77       362\n",
      "           1       0.81      0.72      0.76       386\n",
      "\n",
      "    accuracy                           0.76       748\n",
      "   macro avg       0.77      0.77      0.76       748\n",
      "weighted avg       0.77      0.76      0.76       748\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for sentence in data_imdb.values:\n",
    "    pol = TextBlob(sentence[0]).sentiment.polarity\n",
    "    if(pol>0.1):\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n",
    "\n",
    "\n",
    "print(accuracy_score(data_imdb[\"Sentiment\"].values, pred))\n",
    "print(classification_report(data_imdb[\"Sentiment\"].values, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-specific",
   "metadata": {},
   "source": [
    "As can be seen, using textblob sentiment analyzer makes sense and is able to correctly classify the reviews as positive or negative. The process to build such classifiers requires several steps, namely:\n",
    "\n",
    "- 1 - Pre-Process:\n",
    "    - Tokenize\n",
    "- 2 - Feature Extraction:\n",
    "    - TFIDF model\n",
    "- 3 - Train-Test\n",
    "- 4 - Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "beautiful-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords = list(STOP_WORDS)\n",
    "\n",
    "punct = string.punctuation\n",
    "\n",
    "def text_data_cleaning(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.lower().strip()\n",
    "        else:\n",
    "            temp = token.lower_\n",
    "        \n",
    "        tokens.append(temp)\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords and token not in punct:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens\n",
    "\n",
    "def text_data_cleaning2(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"AUX\", \"ADJ\", \"NOUN\"]:\n",
    "            temp = token.lemma_.lower().strip()\n",
    "            tokens.append(temp)\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords and token not in punct:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000, tokenizer=text_data_cleaning2)\n",
    "\n",
    "cls = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "outside-slope",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\biosignals3\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7666666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.68      0.75        76\n",
      "           1       0.72      0.85      0.78        74\n",
      "\n",
      "    accuracy                           0.77       150\n",
      "   macro avg       0.77      0.77      0.77       150\n",
      "weighted avg       0.78      0.77      0.77       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data_imdb[\"Review\"]\n",
    "y = data_imdb[\"Sentiment\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "X_train_vectorized = vectorizer.transform(X_train)\n",
    "\n",
    "cls.fit(X_train_vectorized, y_train)\n",
    "\n",
    "y_pred = cls.predict(vectorizer.transform(X_test))\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-diary",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "### Using Markovify (Markov Chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "domestic-lender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, when our turn came the occasional cry of surprise, threw up my mind to run short, and I concealed a piece of white satin shoes and a pair of glasses on his overcoat.\n",
      "Holmes grinned at the bell-pull.\n",
      "Briefly, Watson, I am boasting when I coupled it with all the facts should now come to the agency and inquire whether the present moment, as far as the old man at my surprise, the three missing stones.\n",
      "Any injury to it as the most part with poor ignorant folk who know little of the day before, all carefully dried and collected on the table he drew the drawer open.\n",
      "Who were these German people, and what were we going, and what was about to renew her entreaties when a sudden turn to rain, with high collar, black frock-coat, white waistcoat, yellow gloves, patent-leather shoes, and light-coloured gaiters.\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "\n",
    "corpus = open(r\"..\\Data\\shakespear.txt\").read()\n",
    "txt_model = markovify.Text(corpus)\n",
    "\n",
    "for i in range(5):\n",
    "    print(txt_model.make_sentence())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-implement",
   "metadata": {},
   "source": [
    "### Using Xereg or Exrex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "regional-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pppppnn\n",
      "ppppppppnnz\n",
      "ppppppppnnn\n",
      "pppppppppnnnnz\n",
      "pnnnnnnn\n",
      "\n",
      "\n",
      "zrynwgmps@fct.unl.com\n",
      "npbkada@hotmail.com\n",
      "glqaqssu@hotmail.pt\n",
      "hgjsb@gmail.com\n",
      "yiimxqbhct@gmail.com\n",
      "\n",
      "\n",
      "jkoyvpwffn@gmail.com\n",
      "wzwqdy@fct.unl.pt\n",
      "sjmcgzdxllxhtea@gmail.pt\n",
      "rafhbixnnownzd@hotmail.pt\n",
      "ozcpc@gmail.com\n"
     ]
    }
   ],
   "source": [
    "from xeger import Xeger\n",
    "\n",
    "a = Xeger(limit=10)\n",
    "\n",
    "#print 5 different \n",
    "for i in range(5):\n",
    "    print(a.xeger(\"p+n+z?\"))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "pattern = \"[a-z]{5,15}@(gmail|hotmail|fct\\.unl)\\.(com|pt)\"\n",
    "#generating 5 different emails\n",
    "for i in range(5):\n",
    "    print(a.xeger(pattern))\n",
    "\n",
    "print(\"\\n\")\n",
    "    \n",
    "import exrex\n",
    "for i in range(5):\n",
    "    print(exrex.getone(pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-veteran",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "In this example we will provide a simple example taken from \"https://github.com/huggingface/blog/tree/master/notebooks\" to show how to use Transformers for Tokenization and AutoCompletion tasks. Other examples are available here: https://github.com/huggingface/transformers/tree/master/examples and https://github.com/huggingface/transformers/tree/master/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "constant-assembly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "\ttensor([[ 101, 1188, 1110, 1126, 7758, 1859,  102]])\n",
      "token_type_ids:\n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask:\n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1]])\n",
      "Single segment token (str): ['[CLS]', 'I', 'am', 'very', 'disappointed', '.', 'This', 'method', 'is', 'quite', 'expensive', 'to', 'perform', 'token', '##ization', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MODEL_NAME = \"bert-base-cased\"\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokens_pt = tokenizer(\"This is an input example\", return_tensors=\"pt\")\n",
    "for key, value in tokens_pt.items():\n",
    "    print(\"{}:\\n\\t{}\".format(key, value))\n",
    "    \n",
    "single_seg_input = tokenizer(\"I am very disappointed. This method is quite expensive to perform tokenization.\")\n",
    "print(\"Single segment token (str): {}\".format(tokenizer.convert_ids_to_tokens(single_seg_input['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "collaborative-active",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I have been robbed. I should call\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I have been robbed. I should call the police\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I have been robbed. I should call', return_tensors='pt')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=10)\n",
    "\n",
    "print(\"Input:\\n\"+ 100 * '-')\n",
    "print(\"I have been robbed. I should call\")\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-prior",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "#### POS and NER\n",
    "\n",
    "- https://spacy.io/usage/rule-based-matching#dependencymatcher\n",
    "\n",
    "#### Text Summarization\n",
    "- https://python-bloggers.com/2020/09/text-summarization-in-python-with-gensim/\n",
    "- https://medium.com/analytics-vidhya/text-summarization-using-spacy-ca4867c6b744\n",
    "- https://medium.com/analytics-vidhya/nlp-text-summarization-an-overview-bc105810f71e\n",
    "- https://towardsdatascience.com/simple-text-summarization-in-python-bdf58bfee77f\n",
    "- https://predictivehacks.com/text-summarization-in-python-with-gensim/\n",
    "- https://github.com/Jcharis/Natural-Language-ProcessingTutorials/blob/master/NLP_with_SpaCy/Text%20Summarization%20In%20SpaCy.ipynb\n",
    "- https://arxiv.org/pdf/1602.03606.pdf\n",
    "- https://www.aclweb.org/anthology/W04-3252.pdf\n",
    "- https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70\n",
    "- https://arxiv.org/pdf/1707.02268v3.pdf\n",
    "- https://arxiv.org/pdf/1703.09902v1.pdf\n",
    "\n",
    "#### Topic Modelling\n",
    "- https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "- https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "\n",
    "#### Sentiment Analysis\n",
    "- https://stackabuse.com/sentiment-analysis-in-python-with-textblob/\n",
    "- https://kgptalkie.com/amazon-and-imdb-review-sentiment-classification-using-spacy/\n",
    "\n",
    "#### Text Generation\n",
    "- https://github.com/crdoconnor/xeger/blob/master/xeger/xeger.py\n",
    "- https://github.com/asciimoo/exrex\n",
    "- https://github.com/jsvine/markovify\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "#### Transformer\n",
    "- https://github.com/huggingface/transformers/tree/master/notebooks\n",
    "- https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/language_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-sweet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
